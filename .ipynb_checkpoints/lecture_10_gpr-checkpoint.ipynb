{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "% DEFINITIONS\n",
    "\\newcommand{\\bff}{\\mathbf{f}}\n",
    "\\newcommand{\\bm}{\\mathbf{m}}\n",
    "\\newcommand{\\bk}{\\mathbf{k}}\n",
    "\\newcommand{\\bx}{\\mathbf{x}}\n",
    "\\newcommand{\\by}{\\mathbf{y}}\n",
    "\\newcommand{\\bz}{\\mathbf{z}}\n",
    "\\newcommand{\\bA}{\\mathbf{A}}\n",
    "\\newcommand{\\bB}{\\mathbf{B}}\n",
    "\\newcommand{\\bC}{\\mathbf{C}}\n",
    "\\newcommand{\\bD}{\\mathbf{D}}\n",
    "\\newcommand{\\bI}{\\mathbf{I}}\n",
    "\\newcommand{\\bK}{\\mathbf{K}}\n",
    "\\newcommand{\\bL}{\\mathbf{L}}\n",
    "\\newcommand{\\bM}{\\mathbf{M}}\n",
    "\\newcommand{\\bX}{\\mathbf{X}}\n",
    "\\newcommand{\\bY}{\\mathbf{Y}}\n",
    "\\newcommand{\\calX}{\\mathcal{X}}\n",
    "\\newcommand{\\bLambda}{\\boldsymbol{\\Lambda}}\n",
    "\\newcommand{\\bSigma}{\\boldsymbol{\\Sigma}}\n",
    "\\newcommand{\\bmu}{\\boldsymbol{\\mu}}\n",
    "\\newcommand{\\calN}{\\mathcal{N}}\n",
    "\\newcommand{\\calD}{\\mathcal{D}}\n",
    "\\newcommand{\\R}{\\mathbb{R}}\n",
    "\\newcommand{\\E}{\\mathbb{E}}\n",
    "\\newcommand{\\C}{\\mathbb{C}}\n",
    "\\newcommand{\\Rd}{\\R^d}\n",
    "\\newcommand{\\Rdd}{\\R^{d\\times d}}\n",
    "\\newcommand{\\bzero}{\\mathbf{0}}\n",
    "\\newcommand{\\GP}{\\mbox{GP}}\n",
    "% END OF DEFINITIONS\n",
    "$\n",
    "\n",
    "\n",
    "\n",
    "## Introduction to Gaussian process regression  (GPR)\n",
    "\n",
    "Recall, that in the previous lecture we discussed the following :\n",
    "\n",
    "+ What is prior knowledge?\n",
    "\n",
    "+ What is a Gaussian process (GP) ?\n",
    "\n",
    "+ What are the properties of the mean and covariance functions of a Gaussian process and what kind of priors can we encode into a GP through the mean and the covariance kernel? \n",
    "\n",
    "+ How do we sample from a GP ?\n",
    "\n",
    "\n",
    "In this lecture, we shall talk about how we develop response functions to approximate a generic black box computer code (say $f(\\cdot)$) in a manner that makes it compatible with our prior beliefs about the model. We do so, by using Bayes' rule and the Gaussian process regression method. Remember that our goal is to be able to propagate uncertainty in the inputs. \n",
    "\n",
    "We saw in the previous lecture that one's prior knowledge about the response can be modeled in terms of a generic GP. Let that prior state of knowledge be represented as follows: \n",
    "\\begin{equation}\n",
    "f(\\cdot) | m(\\cdot), k(\\cdot, \\cdot) \\sim \\GP\\left(f(\\cdot) | m(\\cdot), k(\\cdot, \\cdot) \\right),\n",
    "\\end{equation}\n",
    "\n",
    "where the terms have their usual meaning i.e., $f(\\cdot)$ is a generic response surface, $m(\\cdot)$ is the prior mean function and $k(\\cdot, \\cdot)$ is the covariance kernel. \n",
    "\n",
    "\n",
    "Now, assume that we make $n$ _measurements_ or _simulations_ at input locations $\\bx_1, \\bx_2, \\cdots, \\bx_n$ such that $\\bx_i \\in \\R^{d}$. The corresponding observed outputs are $y_1, y_2, \\cdots, y_n$, such that $y_i \\in \\R$. We write $\\bX = \\{\\bx_1, \\bx_2, \\cdots, \\bx_n\\}$ and $\\bY=\\{y_1, y_2, \\cdots, y_n\\}$. Abusing mathematical notation slightly, we use the symbol $\\calD$ to denote $\\bX$ and $\\bY$ collectively. We refer to $\\calD$ as the _observed data_. How does the observed data $\\calD$ affect our state of knowledge about the response surface? \n",
    "\n",
    "The answer lies in a straightforward application of Bayes' rule which can be stated as:\n",
    "$$\n",
    "posterior = \\frac{likelihood \\times prior}{marginal \\ likelihood}\n",
    "$$\n",
    "\n",
    "Our state of knowledge about the response function, conditioned upon the observed data, is a new GP which can be expressed as follows: \n",
    "\n",
    "$$\n",
    "f(\\cdot)|\\calD \\sim GP(f(\\cdot)| m^{*} (\\cdot;\\calD) k^{*}(\\cdot, \\cdot; \\calD))\n",
    "$$\n",
    "\n",
    "with mean function: \n",
    "$$\n",
    "m^{*}(\\bx) = m(\\bx) + \\bk(\\bx, \\bX) \\bK(\\bX, \\bX)^{-1}(\\bY-m(\\bX))\n",
    "$$\n",
    "\n",
    "and covariance function:\n",
    "$$\n",
    "k^{*}(\\bx, \\bx') = k(\\bx, \\bx') - \\bk(\\bx, \\bX)\\bK(\\bX, \\bX)^{-1}\\bk(\\bX, \\bx')\n",
    "$$\n",
    "\n",
    "\n",
    "and the _posterior_ of the hyperparameters. \n",
    "\n",
    "If we believe that the observed measurements are noisy, we model this noise as Gaussian with variance $\\sigma_{n}^{2}$ and replace the occurrences of the matrix $\\bK$ with $\\bK + \\sigma_{n}^{2}\\bI$, where $\\bI$ is the identity matrix. \n",
    "\n",
    "Let us see GP Regression in action. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Define python modules needed in this lecture.\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tables as tb\n",
    "from ipywidgets import interactive\n",
    "import math\n",
    "import GPy\n",
    "import numpy as np\n",
    "import scipy\n",
    "import os\n",
    "import sys\n",
    "import sympy\n",
    "from sympy.utilities.autowrap import ufuncify\n",
    "from sympy.interactive import printing\n",
    "printing.init_printing()\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let us again quickly define the squared exponential kernel and a method to compute the covariance matrix as we did in the previous lecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def se_cov(x1, x2, s=1., ell=1.):\n",
    "    \"\"\"\n",
    "    A function the computes the SE covariance:\n",
    "    \n",
    "    :param x1:        One input point (1D array).\n",
    "    :param x2:        Another input point (1D array).\n",
    "    :param s:         The signal strength (> 0).\n",
    "    :param ell:       The length scale(s). Either a positive number or a 1D array of\n",
    "                      positive numbers.\n",
    "    :returns:         The value of the SE covariance evaluated at x1 and x2 with\n",
    "                      signal strength s and length scale(s) ell.\n",
    "    \n",
    "    .. pre::\n",
    "        \n",
    "        + x1 must have the same dimensions as x2\n",
    "        + ell must either be a float or a 1D array with the same dimensions as x1\n",
    "    \"\"\"\n",
    "    tmp = (x1 - x2) / ell\n",
    "    return s ** 2 * math.exp(-0.5 * np.dot(tmp, tmp))\n",
    "\n",
    "def cov_mat(X1, X2, cov_fun=se_cov, **cov_params):\n",
    "    \"\"\"\n",
    "    Compute the cross covariance matrix of `X1` and `X2` for the covariance\n",
    "    function `cov_fun`.\n",
    "    \n",
    "    :param X1:           A matrix of input points (n1 x d)\n",
    "    :param X2:           A matrix of input points (n2 x d)\n",
    "    :param cov_fun:      The covariance function to use\n",
    "    :param cov_param:    Any parameters that we should pass to the covariance\n",
    "                         function `cov_fun`.\n",
    "    \"\"\"\n",
    "    X1 = np.array(X1)\n",
    "    X2 = np.array(X2)\n",
    "    return np.array([[cov_fun(X1[i, :], X2[j, :], **cov_params) for j in xrange(X2.shape[0])]\n",
    "                     for i in xrange(X1.shape[0])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume that we have some scientific simulator. In practice, this simulator is a black box which spits out some output given some input. Here we define the simulator 'solver' as a simple trigonometric expression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "solver = lambda(x): -np.cos(np.pi * x) + np.sin(4. * np.pi * x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us sample some input points and evaluate our 'solver' at these locations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_sim = 10\n",
    "X = np. random.rand(num_sim, 1)\n",
    "Y = solver(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first compute the covariance matrix and use lengthscale 1.0 and signal strength 1.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "K = cov_mat(X, X, cov_fun=se_cov, s=1.0, ell=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define a routine to compute the Cholesky decomposition with addition of a parameter defined as \"nugget\". Essentially this parameter is required to maintain numerical stability. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def cholesky(C, nugget=0):\n",
    "    \"\"\"\n",
    "    Compute the Cholesky decomposition of the matrix, adding a nugget\n",
    "    if necessary.\n",
    "    \n",
    "    :param C:       A semi-positive definite covariance matrix.\n",
    "    :param nugget:  A nugget that should be added to the diagonal of the covariance\n",
    "                    matrix for stability.\n",
    "\n",
    "    :returns:       The lower triangular Cholesky factor.\n",
    "    \"\"\"\n",
    "    CC = C.copy()\n",
    "    while True:\n",
    "        try:\n",
    "            L = np.linalg.cholesky(CC)\n",
    "            return L\n",
    "        except np.linalg.LinAlgError as e:\n",
    "            if nugget == 0:\n",
    "                nugget = 1e-16\n",
    "            else:\n",
    "                nugget *= 10\n",
    "            CC = C + nugget * np.eye(C.shape[0])\n",
    "            L = np.linalg.cholesky(CC)\n",
    "            return L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we compute the Cholesky decomposition: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  1.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00]\n",
      " [  9.96547850e-01   8.30203762e-02   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00]\n",
      " [  9.97151178e-01  -7.49555184e-02   8.43794715e-03   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00]\n",
      " [  9.60680550e-01   2.74860372e-01   3.84668537e-02   8.05965495e-03\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00]\n",
      " [  9.97365248e-01   7.25415016e-02  -5.38440090e-04  -4.59043150e-05\n",
      "    4.87156313e-06   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00]\n",
      " [  8.66927944e-01  -4.52370947e-01   2.01771203e-01  -5.10667705e-02\n",
      "    2.10238021e-02   5.91884409e-03   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00]\n",
      " [  9.91209694e-01  -1.30765013e-01   2.00850304e-02  -6.53236339e-04\n",
      "    1.36879204e-04   1.27553766e-05   3.27696829e-06   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00]\n",
      " [  9.98741898e-01   5.01323489e-02  -1.16727567e-03  -8.42966063e-05\n",
      "    9.89391031e-06   1.17888304e-07   1.75398262e-07   1.75049009e-07\n",
      "    0.00000000e+00   0.00000000e+00]\n",
      " [  9.45696113e-01  -3.11131441e-01   9.30423563e-02  -1.34571868e-02\n",
      "    4.18369114e-03   7.76703322e-04   7.11074092e-05  -1.52563779e-05\n",
      "    2.08726709e-05   0.00000000e+00]\n",
      " [  9.87719329e-01  -1.53997134e-01   2.63410998e-02  -1.21794891e-03\n",
      "    2.70124720e-04   2.81350156e-05   6.36251325e-06  -1.94204491e-07\n",
      "    3.56744712e-07   2.07549159e-07]]\n"
     ]
    }
   ],
   "source": [
    "nugget = 1e-14 * np.eye(K.shape[0])\n",
    "L = np.linalg.cholesky(K+nugget)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
